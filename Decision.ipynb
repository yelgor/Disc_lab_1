{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbc897b-d259-4876-b6d3-3a2ce9878754",
   "metadata": {},
   "source": [
    "#### Виконали: Гринда Юліана, Єлісєєв Гордій"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769a3d1-65a5-4ffc-8e55-47d26e7ceb09",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b2b22-e896-4ec2-b9bb-b984bd72aa9c",
   "metadata": {},
   "source": [
    "The purpose of this laboratory work was to implement the classification through the decision tree: classification of data by these classes. First, we implemented the criterion by which the tree node will be affected: the Gini criterion. \n",
    "\n",
    "The Gini criterion determines the degree of stratification of the dataset, i.e., a single class of data. If the data are in large quantities from different classes, the Gini criterion will be close to 1, but if the data are in large quantities from the same class, the Gini criterion will be close to 0. The same measure will be entropy. \n",
    "\n",
    "The function of separating the data into right and left sons was developed. First, the function must separate the data so that the Gini index of the two groups is smaller than the maternal one. In addition, our separation should be such that Gini’s index for right and left sons is minimal. It can be implemented with a simple search, having the complexity of the code $O(n^2)$, but instead we compute them in an iterative fashion, making this for loop linear $O(n)$ rather than quadratic.\n",
    "\n",
    "The sorting algorithm is performed recursively, increasing the length of the descent until it reaches maximum, distributing the sorted data by the written sorting algorithm.\n",
    "c.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eded9b8-3820-4997-8a29-2d5acdbd4211",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed78c14-0b0e-419b-b6e2-7cd6bb78055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''This class is created to discribe a node in keys of tree ''' \n",
    "    def __init__(self, X, y, gini):\n",
    "        self.X = X #data\n",
    "        self.y = y #class \n",
    "        self.gini = gini #gini\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0 #threshold\n",
    "        self.left = None # left lief\n",
    "        self.right = None # right lief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ffcc7f-08e9-4608-a4a5-9c8d5f071281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def gini(self, groups, classes):\n",
    "        '''\n",
    "        A Gini score gives an idea of how good a split is by how mixed the\n",
    "        classes are in the two groups created by the split.\n",
    "        '''\n",
    "\n",
    "        gini_ = 1\n",
    "        for group in groups:\n",
    "            gini_ -= (group / sum(list(groups))) ** 2\n",
    "        return gini_\n",
    "    \n",
    "    def split_data(self, X, y) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Find the best split for a node.\n",
    "        \"\"\"\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        \n",
    "\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        num_parent = [np.sum(y == c) for c in classes]\n",
    "\n",
    "        best_gini = self.gini(num_parent, classes)\n",
    "\n",
    "        best_idx, best_thr = None, None\n",
    "\n",
    "        for idx in range(X.shape[1]):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "\n",
    "            num_left = [0] * len(np.unique(y))\n",
    "            num_right = num_parent.copy()\n",
    "\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "\n",
    "                gini_left = self.gini(num_left, np.unique(y))\n",
    "                gini_right = self.gini(num_right, np.unique(y))\n",
    "\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "        return best_idx, best_thr\n",
    "    \n",
    "    def build_tree(self, X, y, depth = 0):\n",
    "        \n",
    "        # create a root node\n",
    "        \n",
    "        # recursively split until max depth is not exeeced\n",
    "        \n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        gini = self.gini(num_samples_per_class, np.unique(y))\n",
    "\n",
    "        node = Node(X, y, gini)\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self.split_data(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "\n",
    "                node.left = self.build_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self.build_tree(X_right, y_right, depth + 1)\n",
    "                \n",
    "        return node\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # basically wrapper for build tree / train\n",
    "        \n",
    "        self.n_classes_ = len(set(y))  # classes are assumed to go from 0 to n-1\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self.build_tree(X, y)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        # traverse the tree while there is a child\n",
    "        # and return the predicted class for it, \n",
    "        # note that X_test can be a single sample or a batch\n",
    "\n",
    "        def _predict(self, inputs): \n",
    "            node = self.tree_\n",
    "            while node.left:\n",
    "                if input[node.feature_index] < node.threshold:\n",
    "                    node = node.left\n",
    "                else: \n",
    "                    node.node.right\n",
    "            return node.predicted_class\n",
    "        \n",
    "        return [self._predict(input) for input in X]\n",
    "        \n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \n",
    "        predictions = predict(X_test)\n",
    "        \n",
    "        return sum(predictions == y_test) / len(y_test)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d34863-86e1-4fc6-8ae3-affcf264f93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
